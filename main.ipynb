{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 0. Imports",
   "id": "8eddb7a7c1f1b40c"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-01T15:54:50.508767Z",
     "start_time": "2026-01-01T15:54:50.504637Z"
    }
   },
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from src.dataset_and_loader.loader import get_transform\n",
    "from src.dataset_and_loader.dataset import RecipeDataset\n",
    "from src.dataset_and_loader.loader import dataset_loader\n",
    "from src.model import FusionModel, InfoNCELoss\n",
    "from src.utils.save import load_model\n",
    "from src.pipeline import train_and_validate_model\n",
    "from src.reterival import retrieve\n",
    "from src.utils.plot import show_image\n",
    "from src.utils.utils import count_trainable_params, count_total_params\n",
    "from src.pipeline import encode_texts_in_batches, build_images_in_batches"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1. Hyperparameters",
   "id": "e2e39792427ded01"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T15:54:51.919763Z",
     "start_time": "2026-01-01T15:54:51.889755Z"
    }
   },
   "cell_type": "code",
   "source": [
    "IMAGE_SIZE = 224\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 3e-5 #1e-4\n",
    "TEMPERATURE = 0.05 #0.05\n",
    "MODEL_PATH = \"fusion_model.pth\"\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\""
   ],
   "id": "c0dfa10191f866ba",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2. Dataset and DataLoader",
   "id": "616c713a2201da43"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T15:54:53.723749Z",
     "start_time": "2026-01-01T15:54:53.503366Z"
    }
   },
   "cell_type": "code",
   "source": [
    "base_dir = os.getcwd()\n",
    "csv_path = os.path.join(base_dir, \"data\", \"Food Ingredients and Recipe Dataset with Image Name Mapping.csv\")\n",
    "image_dir = os.path.join(base_dir, \"data\", \"Food Images\")\n",
    "\n",
    "train_transform, val_transform = get_transform(IMAGE_SIZE)\n",
    "full_dataset = RecipeDataset(csv_file=csv_path, image_dir=image_dir, transform=train_transform)\n",
    "train_loader, val_loader = dataset_loader(full_dataset, train_transform, val_transform, BATCH_SIZE)"
   ],
   "id": "284191725ba4fd4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 10800, Val size: 2701\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3. Initializations",
   "id": "e54dc4dd273b8986"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T15:54:59.712277Z",
     "start_time": "2026-01-01T15:54:55.126560Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = FusionModel(device=device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_fun = InfoNCELoss(temperature=TEMPERATURE)"
   ],
   "id": "c82b2410eeb86087",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T15:55:02.004989Z",
     "start_time": "2026-01-01T15:55:02.002187Z"
    }
   },
   "cell_type": "code",
   "source": "model, optimizer, last_epoch = load_model(model, optimizer, path=MODEL_PATH, device=device)",
   "id": "e0f7eba8209ea721",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T15:55:06.446023Z",
     "start_time": "2026-01-01T15:55:06.442963Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if last_epoch > 1:\n",
    "    last_epoch += 1\n",
    "print(f\"last epoch: {last_epoch}\")"
   ],
   "id": "302ddcd80e6340fe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last epoch: 1\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T15:55:08.943802Z",
     "start_time": "2026-01-01T15:55:08.940230Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"Trainable params: {count_trainable_params(model)}\")\n",
    "print(f\"Total params: {count_total_params(model)}\")"
   ],
   "id": "30ef17e2a2c12f62",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 159910976\n",
      "Total params: 159910976\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 4. Train and Validate Model",
   "id": "16e940ae97a13e6f"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2026-01-01T15:55:11.296131Z"
    }
   },
   "cell_type": "code",
   "source": [
    "TRAIN_NEXT_EPOCHS = 5\n",
    "pre_last_epoch = last_epoch\n",
    "train_and_validate_model(\n",
    "    model, train_loader, val_loader,\n",
    "    optimizer, loss_fun, device,\n",
    "    start_epoch=pre_last_epoch,\n",
    "    epochs=TRAIN_NEXT_EPOCHS,\n",
    "    save_path=MODEL_PATH\n",
    ")"
   ],
   "id": "576fd9071cff3531",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total epochs: 5\n",
      "Epoch [1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 675/675 [12:58<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 4.9765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 169/169 [00:59<00:00,  2.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 3.7709\n",
      "Text->Image: R@1 0.020, R@5 0.053, R@10 0.087, MedR 166.000, MRR 0.046\n",
      "Text->LongText: R@1 0.073, R@5 0.217, R@10 0.320, MedR 31.000, MRR 0.152\n",
      "Image->Text: R@1 0.016, R@5 0.057, R@10 0.096, MedR 157.000, MRR 0.046\n",
      "Image->LongText: R@1 0.024, R@5 0.074, R@10 0.122, MedR 97.000, MRR 0.062\n",
      "Model saved to fusion_model.pth\n",
      "Loss saved to metrics/training_and_validation_loss_log.csv\n",
      "Retrieval Metrics saved to metrics/retrieval_metrics_log.csv\n",
      "Epoch [2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|███       | 203/675 [03:24<08:16,  1.05s/it]"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 5. Evaluation metrices and Plot Graph",
   "id": "1b3c4dd341c8ac56"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### a. Train and Validate Loss Graph",
   "id": "e6e34347d39755ec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from src.utils.plot import plot_training_n_validation_loss\n",
    "from src.utils.save import load_training_n_validation_loss\n",
    "\n",
    "epochs, train_loss, val_loss = load_training_n_validation_loss()\n",
    "plot_training_n_validation_loss(epochs, train_loss, val_loss)\n"
   ],
   "id": "bf44873ae608375e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 6. Test Model",
   "id": "180cb5c54a9fe5ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "loaded_model, _, _ = load_model(model, optimizer, path=MODEL_PATH, device=device)",
   "id": "c4aa1606cd84e31c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### a. Dataset Embeddings",
   "id": "9405081865de7fac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "dataset_texts = []\n",
    "dataset_ingredients_instructions = []\n",
    "dataset_metadata = []\n",
    "dataset_images = []\n",
    "\n",
    "for i in tqdm(range(len(full_dataset)), desc=\"Preparing texts & metadata\"):\n",
    "    sample = full_dataset[i]\n",
    "    dataset_texts.append(sample[\"input_text\"])  # title\n",
    "    dataset_ingredients_instructions.append(sample[\"target_text\"])  # ingredients, instructions\n",
    "    dataset_images.append(sample[\"image\"])\n",
    "    dataset_metadata.append(sample[\"metadata\"])\n",
    "# Stack images into a single tensor [N, C, H, W]\n",
    "dataset_images = torch.stack(dataset_images)"
   ],
   "id": "aca8ecaefaa1e748",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# with torch.no_grad():\n",
    "#     dataset_title_embeds = loaded_model.forward_text(dataset_texts)\n",
    "dataset_title_embeds = encode_texts_in_batches(\n",
    "    loaded_model,\n",
    "    dataset_texts,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    device=device,\n",
    "    desc=\"Building titles embeddings:\"\n",
    ")"
   ],
   "id": "c82f5de20fcd8e95",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# with torch.no_grad():\n",
    "#     dataset_ingredients_instructions_embeds = loaded_model.forward_long_text(dataset_ingredients_instructions).to(device)\n",
    "dataset_ingredients_instructions_embeds = encode_texts_in_batches(\n",
    "    loaded_model,\n",
    "    dataset_ingredients_instructions,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    device=device,\n",
    "    desc=\"Building Ingredients and Instructions embeddings: \"\n",
    ")"
   ],
   "id": "776953f2136ba74e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# with torch.no_grad():\n",
    "#   dataset_image_embeds = loaded_model.forward_image(dataset_images).to(device)\n",
    "dataset_image_embeds = build_images_in_batches(\n",
    "    model=loaded_model,\n",
    "    dataset_images=dataset_images,\n",
    "    device=device\n",
    ")"
   ],
   "id": "38ecd5621b6c09e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(dataset_title_embeds.shape)\n",
    "print(dataset_ingredients_instructions_embeds.shape)\n",
    "print(dataset_image_embeds.shape)"
   ],
   "id": "499051cacc89ce47",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### b. Retrieve from your query",
   "id": "b2baa50c9abaf367"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### i. text -> title",
   "id": "53c8e872a73fbd74"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "query_texts = [\"crispy salt and pepper potatoes\"]\n",
    "\n",
    "# Retrieve\n",
    "results = retrieve(\n",
    "    model=loaded_model,\n",
    "    query_texts=query_texts,\n",
    "    query_images=None,\n",
    "    dataset_title_embeds=dataset_title_embeds,\n",
    "    dataset_ingredients_instructions_embeds=None,\n",
    "    dataset_image_embeds=None,\n",
    "    top_k=3,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Example access\n",
    "indices, scores = results[\"text->text\"]\n",
    "print(\"text->ingredients_instructions indices:\", indices)\n",
    "print(\"text->ingredients_instructions scores:\", scores)\n"
   ],
   "id": "d64e35ca67c14775",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Loop through the hits\n",
    "for rank, idx in enumerate(indices[0]):  # 0 because batch size is 1\n",
    "    idx = idx.item()  # convert from tensor to int\n",
    "    sample = full_dataset[idx]\n",
    "    print(f\"{rank + 1}. Score: {scores[0][rank].item():.3f}\")\n",
    "    print(f\"   Title: {sample['metadata']['title']}\")\n",
    "    print(f\"   Ingredients+Instructions: {sample['target_text']}\")"
   ],
   "id": "f7becf9e60f6aaff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### ii. image -> image",
   "id": "2b127d854b5fa852"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "image_name = \"crispy-salt-and-pepper-potatoes-dan-kluger\"\n",
    "image_dir = os.path.join(base_dir, \"data\", \"Food Images\")\n",
    "image_path = os.path.join(image_dir, image_name + \".jpg\")\n",
    "img = Image.open(image_path).convert(\"RGB\")\n",
    "img_tensor = val_transform(img)\n",
    "img_tensor = img_tensor.unsqueeze(0)  # add batch dimension [1, C, H, W]\n",
    "query_images = img_tensor.to(device)\n",
    "\n",
    "# Retrieve\n",
    "results = retrieve(\n",
    "    model=loaded_model,\n",
    "    query_texts=None,\n",
    "    query_images=query_images,\n",
    "    dataset_title_embeds=None,\n",
    "    dataset_ingredients_instructions_embeds=None,\n",
    "    dataset_image_embeds=dataset_image_embeds,\n",
    "    top_k=3,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Example access\n",
    "indices, scores = results[\"image->image\"]\n",
    "print(\"image->image indices:\", indices)\n",
    "print(\"image->image scores:\", scores)"
   ],
   "id": "ac660f21cd66fbe3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Loop through the hits\n",
    "for rank, idx in enumerate(indices[0]):  # 0 because batch size is 1\n",
    "    idx = idx.item()  # convert from tensor to int\n",
    "    sample = full_dataset[idx]\n",
    "    print(f\"{rank + 1}. Score: {scores[0][rank].item():.3f}\")\n",
    "    print(f\"   Title: {sample['metadata']['title']}\")\n",
    "    print(f\"   Image Path: {sample['metadata']['image_path']}\")\n",
    "    show_image(sample['metadata']['image_path'])"
   ],
   "id": "b64f8e36741798f4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### iii. text -> image",
   "id": "c0b1bf81c030c3f1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "query_texts = [\"crispy salt and pepper potatoes\"]\n",
    "# query_texts = [\"pizza\"]\n",
    "# query_texts = [\"Salt-and-Pepper Fish\"]\n",
    "\n",
    "# Retrieve\n",
    "results = retrieve(\n",
    "    model=loaded_model,\n",
    "    query_texts=query_texts,\n",
    "    query_images=None,\n",
    "    dataset_title_embeds=None,\n",
    "    dataset_ingredients_instructions_embeds=None,\n",
    "    dataset_image_embeds=dataset_image_embeds,\n",
    "    top_k=3,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Example access\n",
    "indices, scores = results[\"text->image\"]\n",
    "print(\"text->image indices:\", indices)\n",
    "print(\"text->image scores:\", scores)"
   ],
   "id": "775c2a364e3ed7b0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Loop through the hits\n",
    "for rank, idx in enumerate(indices[0]):  # 0 because batch size is 1\n",
    "    idx = idx.item()  # convert from tensor to int\n",
    "    sample = full_dataset[idx]\n",
    "    print(f\"{rank + 1}. Score: {scores[0][rank].item():.3f}\")\n",
    "    print(f\"   Title: {sample['metadata']['title']}\")\n",
    "    print(f\"   Image Path: {sample['metadata']['image_path']}\")\n",
    "    show_image(sample['metadata']['image_path'])"
   ],
   "id": "4568ad0bfc2998ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### iv. image -> text",
   "id": "f82659b71cd39678"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "image_name = \"miso-butter-roast-chicken-acorn-squash-panzanella\"\n",
    "# image_name = \"pan-seared-salt-and-pepper-fish\"\n",
    "\n",
    "image_dir = os.path.join(base_dir, \"data\", \"Food Images\")\n",
    "image_path = os.path.join(image_dir, image_name + \".jpg\")\n",
    "img = Image.open(image_path).convert(\"RGB\")\n",
    "img_tensor = val_transform(img)\n",
    "img_tensor = img_tensor.unsqueeze(0)  # add batch dimension [1, C, H, W]\n",
    "query_images = img_tensor.to(device)\n",
    "print(query_images.shape)\n",
    "\n",
    "# Retrieve\n",
    "results = retrieve(\n",
    "    model=loaded_model,\n",
    "    query_texts=None,\n",
    "    query_images=query_images,\n",
    "    dataset_title_embeds=dataset_title_embeds,\n",
    "    dataset_ingredients_instructions_embeds=None,\n",
    "    dataset_image_embeds=None,\n",
    "    top_k=3,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Example access\n",
    "indices, scores = results[\"image->text\"]\n",
    "print(\"image->text indices:\", indices)\n",
    "print(\"image->text scores:\", scores)"
   ],
   "id": "51a46824cab300ca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Loop through the hits\n",
    "for rank, idx in enumerate(indices[0]):  # 0 because batch size is 1\n",
    "    idx = idx.item()  # convert from tensor to int\n",
    "    sample = full_dataset[idx]\n",
    "    print(f\"{rank + 1}. Score: {scores[0][rank].item():.3f}\")\n",
    "    print(f\"   Title: {sample['metadata']['title']}\")\n",
    "    print(f\"   Image Path: {sample['metadata']['image_path']}\")\n",
    "    show_image(sample['metadata']['image_path'])"
   ],
   "id": "33b31d6fc4874a5",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
